# ğŸšŒ BUS_Prediction

This project predicts bus status using various deep learning models, including LSTM, GRU, Transformer, and a custom BERT-based model.

## ğŸ“‹ Project Overview

The project uses historical bus status data to predict future bus statuses. Each model is trained on sequences of bus status observations and learns to predict the next status in the sequence.

## ğŸ“ Project Structure

- **`train_comparison.py`**: ğŸ‹ï¸ Main training script that processes a single Excel file (`dataset/Status_100.xlsx`), trains all models sequentially, and saves individual model results to Excel files.
- **`plot_model_history.py`**: ğŸ“Š Visualization script to generate performance plots from the result Excel files.
- **`dataset/`**: ğŸ“‚ Directory containing input data (Excel files).
- **`result_*.xlsx`**: ğŸ“ˆ Individual Excel files containing epoch-by-epoch performance metrics for each model (e.g., `result_LSTM.xlsx`, `result_GRU.xlsx`).
- **`requirements.txt`**: ğŸ“¦ Python dependencies.

## âœ¨ Key Features

- **ğŸ¯ Single File Input**: Processes one Excel file at a time (configurable in the script).
- **ğŸ“… Time-Based Splitting**: Training and test data are split based on dates (80/20 split).
- **ğŸ”¢ Sequence Padding**: Sequences shorter than the fixed length (10) are padded with -1.
- **ğŸ”„ Cumulative Prediction**: 
    - First bus predicts second bus ğŸšŒ â†’ ğŸšŒ
    - First + Second buses predict third bus ğŸšŒğŸšŒ â†’ ğŸšŒ
    - And so on...
- **ğŸ“„ Individual Model Results**: Each model's training history is saved to a separate Excel file.

## ğŸŒŸ Environment Setup

This project uses the Conda environment named **"Normal"**.

1. **Ensure Conda is installed.** ğŸ
2. **Activate the environment:**
    ```bash
    conda activate Normal
    ```

## ğŸ“¦ Installation

Install the required dependencies:

```bash
pip install -r requirements.txt
```

## ğŸš€ Usage

### 1. ğŸ‹ï¸ Train Models

To train all models (LSTM, GRU, Transformer, BERT) on the configured dataset:

```bash
conda run -n Normal python train_comparison.py
```

**What happens:**
- ğŸ“¥ Loads data from `dataset/Status_100.xlsx`.
- âœ‚ï¸ Splits data by date (first 80% for training, last 20% for testing).
- ğŸ“ Trains each model for the configured number of epochs.
- ğŸ’¾ Saves model checkpoints (e.g., `bus_lstm_model.pth`).
- ğŸ“Š Saves training history to individual Excel files (e.g., `result_LSTM.xlsx`).

### 2. ğŸ“Š Visualize Results

To generate performance comparison plots from the result files:

```bash
conda run -n Normal python plot_model_history.py
```

This creates `model_comparison_plot.png` showing Train/Test Loss and Accuracy for all models across epochs. ğŸ“ˆ

## âš™ï¸ Configuration

You can modify the following parameters in `train_comparison.py`:

- **`DATA_FILE`**: ğŸ“‚ Path to the input Excel file (default: `'./dataset/Status_100.xlsx'`)
- **`SEQUENCE_LENGTH`**: ğŸ”¢ Fixed sequence length with padding (default: `10`)
- **`BATCH_SIZE`**: ğŸ“¦ Training batch size (default: `32`)
- **`HIDDEN_SIZE`**: ğŸ§  Hidden layer size for LSTM/GRU/BERT (default: `256`)
- **`NUM_LAYERS`**: ğŸ—ï¸ Number of layers (default: `3`)
- **`LEARNING_RATE`**: ğŸ“‰ Learning rate (default: `0.001`)
- **`NUM_EPOCHS`**: ğŸ”„ Number of training epochs (default: `50`)
- **`TRAIN_SPLIT_RATIO`**: âœ‚ï¸ Ratio for train/test split by date (default: `0.8`)

## ğŸ¤– Models Implemented

- **LSTM** (Long Short-Term Memory) ğŸ§ 
- **GRU** (Gated Recurrent Unit) ğŸ”„
- **Transformer** (Encoder-only with positional encoding) ğŸ¤–
- **BERT** (Custom implementation using Hugging Face configuration) ğŸ“š
